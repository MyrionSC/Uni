{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 2\n",
    "## Part A: Linear regression\n",
    "\n",
    "***\n",
    "\n",
    "Our aim for this part of the workshop is to fit a linear model from scratch—relying only on the `numpy` library. We'll experiment with two implementations: one based on iterative updates (coordinate descent) and another based on linear algebra. Finally, to check the correctness of our implementation, we'll compare its output to the output of `sklearn`.\n",
    "\n",
    "Firstly we will import the relevant libraries (`numpy`, `matplotlib`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what a command does simply type `object?`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Review\n",
    "In lectures, we saw that a linear model can be expressed as:\n",
    "$$y = w_0 + \\sum_{j = 1}^{m} w_j x_j = \\mathbf{w} \\cdot \\mathbf{x} $$\n",
    "where \n",
    "\n",
    "* $y$ is the *target variable*;\n",
    "* $\\mathbf{x} = [x_1, \\ldots, x_m]$ is a vector of *features* (we define $x_0 = 1$); and\n",
    "* $\\mathbf{w} = [w_0, \\ldots, w_m]$ are the *weights*.\n",
    "\n",
    "To fit the model, we *minimise* the empirical risk with respect to $\\vec{w}$. In this simplest case (square loss), this amounts to minimising the sum of squared residuals:\n",
    "\n",
    "$$SSR(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2$$\n",
    "\n",
    "**Note:** For simplicity, we'll consider the case $m = 1$ (i.e. only one feature excluding the intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data set\n",
    "We'll be working with some data from the Olympics—the gold medal race times for marathon winners from 1896 to 2012. The code block below reads the data into a numpy array of floats, and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1896.            4.47083333]\n",
      " [1900.            4.46472926]\n",
      " [1904.            5.22208333]\n",
      " [1908.            4.15467867]\n",
      " [1912.            3.90331675]\n",
      " [1920.            3.56951267]\n",
      " [1924.            3.82454477]\n",
      " [1928.            3.62483707]\n",
      " [1932.            3.59284275]\n",
      " [1936.            3.53880792]\n",
      " [1948.            3.67010309]\n",
      " [1952.            3.39029111]\n",
      " [1956.            3.43642612]\n",
      " [1960.            3.20583007]\n",
      " [1964.            3.13275665]\n",
      " [1968.            3.32819844]\n",
      " [1972.            3.13583758]\n",
      " [1976.            3.0789588 ]\n",
      " [1980.            3.10581822]\n",
      " [1984.            3.06552909]\n",
      " [1988.            3.09357349]\n",
      " [1992.            3.16111704]\n",
      " [1996.            3.14255244]\n",
      " [2000.            3.08527867]\n",
      " [2004.            3.10265829]\n",
      " [2008.            2.99877553]\n",
      " [2012.            3.03392977]]\n"
     ]
    }
   ],
   "source": [
    "# CSV file with variables YEAR,TIME\n",
    "csv = \"\"\"1896,4.47083333333333\n",
    "1900,4.46472925981123\n",
    "1904,5.22208333333333\n",
    "1908,4.1546786744085\n",
    "1912,3.90331674958541\n",
    "1920,3.5695126705653\n",
    "1924,3.8245447722874\n",
    "1928,3.62483706600308\n",
    "1932,3.59284275388079\n",
    "1936,3.53880791562981\n",
    "1948,3.6701030927835\n",
    "1952,3.39029110874116\n",
    "1956,3.43642611683849\n",
    "1960,3.2058300746534\n",
    "1964,3.13275664573212\n",
    "1968,3.32819844373346\n",
    "1972,3.13583757949204\n",
    "1976,3.07895880238575\n",
    "1980,3.10581822490816\n",
    "1984,3.06552909112454\n",
    "1988,3.09357348817\n",
    "1992,3.16111703598373\n",
    "1996,3.14255243512264\n",
    "2000,3.08527866650867\n",
    "2004,3.1026582928467\n",
    "2008,2.99877552632618\n",
    "2012,3.03392977050993\"\"\"\n",
    "\n",
    "# Read into a numpy array (as floats)\n",
    "olympics = np.genfromtxt(io.BytesIO(csv.encode()), delimiter=\",\")\n",
    "print(olympics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the race time as the *target variable* $y$ and the year of the race as the only non-trivial *feature* $x = x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = olympics[:, 0:1]\n",
    "y = olympics[:, 1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting $y$ vs $x$, we see that a linear model could be a decent fit for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'rx')\n",
    "plt.ylabel(\"y (Race time)\")\n",
    "plt.xlabel(\"x (Year of race)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterative solution (coordinate descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding out the sum of square residuals for this simple case (where $\\mathbf{w}=[w_0, w_1]$) we have:\n",
    "$$SSR(w_0, w_1) = \\sum_{i=1}^{n}(y_i - w_0 - w_1 x_i)^2$$\n",
    "Let's start with an initial guess for the slope $w_1$ (which is clearly negative from the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the maximum likelihood update, we get the following estimate for the intercept $w_0$:\n",
    "$$w_0 = \\frac{\\sum_{i=1}^{n}(y_i-w_1 x_i)}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w0(x, y, w1):\n",
    "    return ... # fill in\n",
    "\n",
    "w0 = update_w0(x, y, w1)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can update $w_1$ based on this new estimate of $w_0$:\n",
    "$$w_1 = \\frac{\\sum_{i=1}^{n} (y_i - w_0) \\times x_i}{\\sum_{i=1}^{n} x_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w1(x, y, w0):\n",
    "    return ... # fill in\n",
    "\n",
    "w1 = update_w1(x, y, w0)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the quality of fit for these values for the weights $w_0$ and $w_1$. We create a vector of \"test\" values `x_test` and a function to compute the predictions according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(1890, 2020)[:, None]\n",
    "\n",
    "def predict(x_test, w0, w1): \n",
    "    return ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the test predictions with a blue line on the same plot as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(x_test, y_test, x, y): \n",
    "    plt.plot(x_test, y_test, 'b-')\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylabel(\"y (Race time)\")\n",
    "    plt.xlabel(\"x (Year of race)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute the sum of square residuals $SSR(w_0,w_1)$ on the training set to measure the goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SSR(x, y, w0, w1): \n",
    "    return ... # fill in\n",
    "\n",
    "print(compute_SSR(x, y, w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious from the plot that the fit isn't very good. \n",
    "We must repeat the alternating parameter updates many times before the algorithm converges to the optimal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10000):\n",
    "    w1 = update_w1(x, y, w0) \n",
    "    w0 = update_w0(x, y, w1) \n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration #{}: SSR = {}\".format(i, compute_SSR(x, y, w0, w1)))\n",
    "print(\"Final estimates: w0 = {}; w1 = {}\".format(w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4cf509194521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_fit' is not defined"
     ]
    }
   ],
   "source": [
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does more than 10 iterations considerably improve fit in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear algebra solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lectures, we saw that it's possible to solve for the optimal weights $\\mathbf{w}^\\star$ analytically. The solution is\n",
    "$$\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "where\n",
    "$$\\mathbf{X} = \\begin{pmatrix} \n",
    "        1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \n",
    "    \\end{pmatrix} \n",
    "  \\quad \\text{and} \\quad \n",
    "  \\mathbf{y} = \\begin{pmatrix} \n",
    "          y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We construct $\\mathbf{X}$ in the code block below, remembering to include the $x_0 = 1$ column for the bias (intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 1.896e+03]\n",
      " [1.000e+00 1.900e+03]\n",
      " [1.000e+00 1.904e+03]\n",
      " [1.000e+00 1.908e+03]\n",
      " [1.000e+00 1.912e+03]\n",
      " [1.000e+00 1.920e+03]\n",
      " [1.000e+00 1.924e+03]\n",
      " [1.000e+00 1.928e+03]\n",
      " [1.000e+00 1.932e+03]\n",
      " [1.000e+00 1.936e+03]\n",
      " [1.000e+00 1.948e+03]\n",
      " [1.000e+00 1.952e+03]\n",
      " [1.000e+00 1.956e+03]\n",
      " [1.000e+00 1.960e+03]\n",
      " [1.000e+00 1.964e+03]\n",
      " [1.000e+00 1.968e+03]\n",
      " [1.000e+00 1.972e+03]\n",
      " [1.000e+00 1.976e+03]\n",
      " [1.000e+00 1.980e+03]\n",
      " [1.000e+00 1.984e+03]\n",
      " [1.000e+00 1.988e+03]\n",
      " [1.000e+00 1.992e+03]\n",
      " [1.000e+00 1.996e+03]\n",
      " [1.000e+00 2.000e+03]\n",
      " [1.000e+00 2.004e+03]\n",
      " [1.000e+00 2.008e+03]\n",
      " [1.000e+00 2.012e+03]]\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can express $\\mathbf{w}^\\star$ explicitly in terms of the matrix inverse $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$, this isn't an efficient way to compute $\\mathbf{w}$ numerically. It is better instead to solve the following system of linear equations:\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}^\\star = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "This can be done in numpy using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.99413059e+00]\n",
      " [3.57460118e-03]]\n"
     ]
    }
   ],
   "source": [
    "w = ((X.T.dot(X))**-1).dot(X.T).dot(y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this solution, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = w\n",
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should verify that the sum of squared residuals $SSR(w_0, w_1)$, match or beats the earlier iterative result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_SSR(x, y, w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The error we computed above is the *training* error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. In later worksheets we'll assess the generalization ability of models using held-out evaluation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Solving using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a good understanding of what's going on under the hood, you can use the functionality in `sklearn` to solve linear regression problems you encounter in the future. Using the `LinearRegression` module, fitting a linear regression model becomes a one-liner as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58bf509178d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` module provides access to the bias weight $w_0$ under the `intercept_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the non-bias weights under the `coef_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check that these results match the solution you obtained previously. Note that sklearn also uses a numerical linear algebra solver under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
